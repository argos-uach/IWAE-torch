{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import sys\n",
    "\n",
    "# local imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append('../src')\n",
    "from models import VariationalAutoencoder, ImportanceWeightedAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "path_to_datasets = '~/datasets'\n",
    "mnist_train_data = torchvision.datasets.MNIST(path_to_datasets, train=True, download=True,\n",
    "                                              transform=torchvision.transforms.ToTensor())\n",
    "mnist_test_data = torchvision.datasets.MNIST(path_to_datasets, train=False, download=True,\n",
    "                                             transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(6, 1), tight_layout=True)\n",
    "for i in range(10):\n",
    "    image, label = mnist_train_data[i]\n",
    "    ax[i].imshow(image.numpy()[0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = list(range(len(mnist_train_data)))\n",
    "idx = list(range(10000))\n",
    "#np.random.shuffle(idx)\n",
    "split = int(0.7*len(idx))\n",
    "\n",
    "train_loader = DataLoader(mnist_train_data, batch_size=32, drop_last=True,\n",
    "                          sampler=SubsetRandomSampler(idx[:split]))\n",
    "\n",
    "valid_loader = DataLoader(mnist_train_data, batch_size=256, drop_last=True,\n",
    "                          sampler=SubsetRandomSampler(idx[split:]))\n",
    "\n",
    "test_loader = DataLoader(mnist_test_data, batch_size=256, drop_last=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model (or skip this and load last model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "model = VariationalAutoencoder(latent_dim=2, data_dim=28*28)\n",
    "#model = ImportanceWeightedAutoencoder(latent_dim=2, data_dim=28*28)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def normalize_data(x):\n",
    "    x = x.reshape(-1, 28*28)\n",
    "    x = (x - torch.mean(x, dim=1, keepdim=True))/torch.std(x, dim=1, keepdim=True)\n",
    "    return x\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    epoch_loss = 0.0\n",
    "    for x, label in train_loader:        \n",
    "        x = normalize_data(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.negELBO(x, mc_samples=10)[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    # Log and checkpoint\n",
    "    if np.mod(epoch, 10) == 0:\n",
    "        print(f\"{epoch} {epoch_loss}\")\n",
    "        torch.save({'current_epoch': epoch, 'model_state_dict': model.state_dict()}, \n",
    "                    '../checkpoints/mnist_vae_last.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImportanceWeightedAutoencoder(latent_dim=2, data_dim=28*28)\n",
    "model.load_state_dict(torch.load('../checkpoints/mnist_vae_spherical_100.pt')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(tight_layout=True)\n",
    "\n",
    "all_q_mu, all_q_std = torch.tensor([]), torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for x, label in test_loader:\n",
    "        x = normalize_data(x)\n",
    "        q_mu, q_logvar = model.encode(x)\n",
    "        all_q_mu = torch.cat((all_q_mu, q_mu))\n",
    "        all_q_std = torch.cat((all_q_std.cpu(), (0.5*q_logvar.cpu()).exp()))\n",
    "all_q_mu = all_q_mu.numpy()\n",
    "all_q_std = all_q_std.numpy()\n",
    "\n",
    "for digit in range(10):\n",
    "    mask = mnist_test_data.targets == digit\n",
    "    ax.errorbar(all_q_mu[mask, 0], all_q_mu[mask, 1],\n",
    "                all_q_std[mask, 0], all_q_std[mask, 1], fmt='none',\n",
    "                alpha=0.5, cmap=plt.cm.tab10, label=str(digit))\n",
    "ax.set_xlabel(r'$z_1$')\n",
    "ax.set_ylabel(r'$z_2$')\n",
    "ax.set_title('MNIST VAE latent space visualization')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 10, figsize=(8, 4), tight_layout=True, sharey=True)\n",
    "for ax_ in ax.ravel():\n",
    "    ax_.get_xaxis().set_ticks([])\n",
    "    ax_.get_yaxis().set_ticks([])\n",
    "\n",
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        x = normalize_data(mnist_test_data.__getitem__(i)[0].unsqueeze(0))\n",
    "        p_parameters, q_parameters, z = model.forward(x)\n",
    "        xhat = model.sample_decoder(p_parameters, mc_samples=3).reshape(-1, 28, 28).numpy()\n",
    "        dec_mu, dec_logvar = p_parameters\n",
    "        dec_mu = dec_mu.reshape(28, 28).cpu().numpy()\n",
    "        #dec_std = (0.5*dec_logvar).exp().reshape(-1, 28, 28) #diagonal\n",
    "        dec_std = (0.5*dec_logvar).exp().repeat(28, 28).cpu().numpy() #spherical\n",
    "    \n",
    "    ax[0, i].imshow(x.reshape(28, 28), cmap=plt.cm.Greys_r) # Data\n",
    "    ax[0, 0].set_ylabel('Data')\n",
    "    ax[1, i].imshow(dec_mu, cmap=plt.cm.Greys_r) # Mean\n",
    "    ax[1, 0].set_ylabel(r'$\\mu$')\n",
    "    ax[2, i].imshow(dec_std, cmap=plt.cm.Greys_r) # Std\n",
    "    ax[2, 0].set_ylabel(r'$\\sigma$')\n",
    "    for j in range(3): # 3 realizations N(Mean, Std^2)\n",
    "        ax[3+j, i].imshow(xhat[j], cmap=plt.cm.Greys_r)\n",
    "        ax[3+j, 0].set_ylabel(f'$\\mu + \\sigma\\epsilon_{j+1}$')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
